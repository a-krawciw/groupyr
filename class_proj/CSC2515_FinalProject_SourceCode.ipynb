{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdUzgR61qRQE",
    "outputId": "3da16e80-92fe-4456-df8a-897740e48e75"
   },
   "outputs": [],
   "source": [
    "!pip install groupyr\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import groupyr as gl\n",
    "import sklearn as sk\n",
    "import sklearn.multiclass as MC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j13zrsiuYotV"
   },
   "outputs": [],
   "source": [
    "#Input Data\n",
    "def import_data(): \n",
    "  Code import\n",
    "  drive.mount('/content/drive')\n",
    "  os.chdir(\"/content/drive/My Drive/CSC2515/FinalProject\")\n",
    "  kid_data = pd.read_csv('/data.csv',delimiter=';')\n",
    "  return kid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byc521BhYr2A"
   },
   "outputs": [],
   "source": [
    "#Data Processing\n",
    "def mark_as_categorical(dataframe: pd.DataFrame, category: str):\n",
    "    dataframe[category] = dataframe[category].astype('category')\n",
    "\n",
    "def get_categories(dataframe: pd.DataFrame):\n",
    "    return [col for col in dataframe.select_dtypes(include=\"category\")]\n",
    "\n",
    "def create_groups_from_1hot(data_frame: pd.DataFrame):\n",
    "    expanded = pd.get_dummies(data_frame)\n",
    "    groups = {col: [] for col in data_frame}\n",
    "    for idx, col in enumerate(expanded):\n",
    "        category = col.split(\"_\")[0]\n",
    "        groups[category].append(idx)\n",
    "\n",
    "    groups = {k: np.array(v) for k, v in groups.items()}\n",
    "    return expanded, groups\n",
    "\n",
    "def expand_data(data):  \n",
    "    mark_as_categorical(data, \"MaritalStatus\")\n",
    "    mark_as_categorical(data, \"ApplicationMode\")\n",
    "    mark_as_categorical(data, \"ApplicationOrder\")\n",
    "    mark_as_categorical(data, \"TimeOfDay\")\n",
    "    mark_as_categorical(data, \"PreviousQualification\")\n",
    "    mark_as_categorical(data, \"Nationality\")\n",
    "    mark_as_categorical(data, \"MotherQualification\")\n",
    "    mark_as_categorical(data, \"FatherQualification\")\n",
    "    mark_as_categorical(data, \"MotherOccupation\")\n",
    "    mark_as_categorical(data, \"FatherOccupation\")\n",
    "    mark_as_categorical(data, \"Course\")\n",
    "\n",
    "    target = data.get(\"Target\").replace(['Dropout', 'Graduate', 'Enrolled'], [0, 1, 2]).astype(float)\n",
    "\n",
    "    #Scaling\n",
    "    for col in data.select_dtypes(include=[\"float64\",'int'], exclude=\"category\"):\n",
    "        data[col] /= data[col].max()\n",
    "\n",
    "    #Hot 1 and Grouping \n",
    "    expanded_X, group_idxs = create_groups_from_1hot(data.drop(columns=\"Target\"))\n",
    "\n",
    "    return expanded_X, group_idxs, target\n",
    "\n",
    "def stage_data(data, stage):   \n",
    "    # staged data feed\n",
    "    if stage == 'sem2':\n",
    "      features = data\n",
    "    elif stage == 'sem1':\n",
    "      features = data.drop(columns = ['Curricular units 2nd sem (credited)',\n",
    "  'Curricular units 2nd sem (enrolled)',\n",
    "  'Curricular units 2nd sem (evaluations)',\n",
    "  'Curricular units 2nd sem (approved)',\n",
    "  'Curricular units 2nd sem (grade)',\n",
    "  'Curricular units 2nd sem (without evaluations)'])\n",
    "    elif stage == 'registration':\n",
    "      features = data.drop(columns = ['Curricular units 1st sem (credited)',\n",
    "  'Curricular units 1st sem (enrolled)',\n",
    "  'Curricular units 1st sem (evaluations)',\n",
    "  'Curricular units 1st sem (approved)',\n",
    "  'Curricular units 1st sem (grade)',\n",
    "  'Curricular units 1st sem (without evaluations)',\n",
    "  'Curricular units 2nd sem (credited)',\n",
    "  'Curricular units 2nd sem (enrolled)',\n",
    "  'Curricular units 2nd sem (evaluations)',\n",
    "  'Curricular units 2nd sem (approved)',\n",
    "  'Curricular units 2nd sem (grade)',\n",
    "  'Curricular units 2nd sem (without evaluations)'])\n",
    "  \n",
    "    expanded_X, group_idxs, target = expand_data(features)\n",
    "    X = expanded_X.to_numpy(np.float64)\n",
    "    y = target.to_numpy(np.float64)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.2)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test, group_idxs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08OpUINZwxi9"
   },
   "outputs": [],
   "source": [
    "#EDA \n",
    "\n",
    "def reorder_data(data): \n",
    "  # re-order\n",
    "  data = data[['MaritalStatus',\n",
    "  'Nationality',\n",
    "  'Displaced',\n",
    "  'Gender',\n",
    "  'Age',  \n",
    "  'International',\n",
    "  'MotherQualification',\n",
    "  'FatherQualification',\n",
    "  'MotherOccupation',\n",
    "  'FatherOccupation',\n",
    "  'SpecialNeeds',\n",
    "  'Debtor',\n",
    "  'TuitionPaid',\n",
    "  'Scholarship',\n",
    "  'Unemployment rate',\n",
    "  'Inflation rate',\n",
    "  'GDP',\n",
    "  'ApplicationMode',\n",
    "  'ApplicationOrder',\n",
    "  'Course',\n",
    "  'TimeOfDay',\n",
    "  'PreviousQualification',\n",
    "  'PreviousGrade',\n",
    "  'AdmissionGrade',\n",
    "  'Curricular units 1st sem (credited)',\n",
    "  'Curricular units 1st sem (enrolled)',\n",
    "  'Curricular units 1st sem (evaluations)',\n",
    "  'Curricular units 1st sem (approved)',\n",
    "  'Curricular units 1st sem (grade)',\n",
    "  'Curricular units 1st sem (without evaluations)',\n",
    "  'Curricular units 2nd sem (credited)',\n",
    "  'Curricular units 2nd sem (enrolled)',\n",
    "  'Curricular units 2nd sem (evaluations)',\n",
    "  'Curricular units 2nd sem (approved)',\n",
    "  'Curricular units 2nd sem (grade)',\n",
    "  'Curricular units 2nd sem (without evaluations)', \n",
    "  'Target']]\n",
    "    \n",
    "  # correlation matrix\n",
    "  plt.subplots(figsize=(10,10))\n",
    "  corr_matrix = data.corr(method='pearson',)\n",
    "  sn.heatmap(corr_matrix, cmap='PiYG',vmin=-1)\n",
    "  return data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IigX4JJJYvqo"
   },
   "outputs": [],
   "source": [
    "#from scipy.stats.stats import ModeResult\n",
    "#Models\n",
    "\n",
    "#Metrics Calculation\n",
    "def standard_metrics(model, X_test, y_test,X_train,y_train,coef,best_hyperparams,label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = sk.metrics.f1_score(y_test, y_pred, average=None)\n",
    "    f1_mean = np.mean(f1)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    df = pd.DataFrame([[f1,f1_mean, train_score, test_score,coef,best_hyperparams,label]], columns=['f1','f1 mean','train_score', 'test_score','feat','best parameters','model_type'])\n",
    "    print(\"Confusion matrix for\", label)\n",
    "    print(sk.metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Logistic Regression (Group/L1/L2/LR) \n",
    "def Groupyr_model(l1_ratios,group,X_train,Y_train,alpha = None):\n",
    "    model_groupyr = MC.OneVsRestClassifier(gl.LogisticSGLCV(l1_ratio = l1_ratios,\\\n",
    "                                                            groups = group,\\\n",
    "                                                            alphas= alpha,\\\n",
    "                                                            scoring = 'f1_macro',\\\n",
    "                                                            n_jobs=-1,\\\n",
    "                                                            random_state = 1,\\\n",
    "                                                            max_iter = 1000,\\\n",
    "                                                            suppress_solver_warnings=False)).fit(X_train,Y_train)\n",
    "                                                 \n",
    "    return model_groupyr\n",
    "\n",
    "def SVM_model(X_train,y_train,hyper_params):\n",
    "\n",
    "    svm_ovr = MC.OneVsRestClassifier(SVC(random_state = 1))\n",
    "    svm_model_cv = GridSearchCV(estimator = svm_ovr,\\\n",
    "                        param_grid = hyper_params, \\\n",
    "                        scoring= 'f1_macro',\\\n",
    "                        verbose = 1,\\\n",
    "                        n_jobs = -1).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    return svm_model_cv\n",
    "\n",
    "def Grad_model(X_train,y_train,hyper_params):\n",
    "\n",
    "    grad_ovr = MC.OneVsRestClassifier(GradientBoostingClassifier(random_state = 1))\n",
    "    grad_cv = GridSearchCV(estimator= grad_ovr ,\\\n",
    "                           scoring = 'f1_macro',\\\n",
    "                           param_grid = hyper_params,\\\n",
    "                           verbose = 1,\n",
    "                           n_jobs = -1 ).fit(X_train,y_train)\n",
    "    return grad_cv\n",
    "\n",
    "def MLP_model(X_train,y_train,hyper_params):\n",
    "    mlp_gs = MC.OneVsRestClassifier(MLPClassifier(random_state=1,max_iter = 1500))\n",
    "    clf = GridSearchCV(mlp_gs, hyper_params, n_jobs=-1, cv=6,scoring = 'f1_macro').fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def RF_model(X_train,y_train,hyper_params):\n",
    "    \n",
    "    rf_ovr = MC.OneVsRestClassifier(RandomForestClassifier(random_state = 1))\n",
    "    rf_cv = GridSearchCV(estimator = rf_ovr,\\\n",
    "                         param_grid = hyper_params, \\\n",
    "                         scoring = 'f1_macro',\\\n",
    "                         cv = 5,\\\n",
    "                         n_jobs =-1, \\\n",
    "                         verbose = 1).fit(X_train,y_train)\n",
    "    return rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIHw-zWe1NNU",
    "outputId": "67f34469-5924-4243-e9aa-bcd9ab7c68ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main \n",
    "\n",
    "kid_data = import_data()\n",
    "data = kid_data\n",
    "data = reorder_data(kid_data)\n",
    "stages = ['registration','sem1','sem2']\n",
    "st_metrics = pd.DataFrame() \n",
    "\n",
    "for stage in stages:\n",
    "    X_train,X_test,y_train,y_test, group_idxs = stage_data(data,stage)\n",
    "\n",
    "    #Logistic Regress(LR,L1,L2,Group) \n",
    "    \n",
    "    LR = Groupyr_model(l1_ratios=1,group=None,X_train=X_train,Y_train=y_train,alpha = [0])\n",
    "    L1 = Groupyr_model(l1_ratios=1,group=None,X_train=X_train,Y_train=y_train)\n",
    "    L2 = Groupyr_model(l1_ratios=0,group=None,X_train=X_train,Y_train=y_train)\n",
    "    Group = Groupyr_model(l1_ratios=0,group=list(group_idxs.values()),X_train=X_train,Y_train=y_train)\n",
    "    \n",
    "    print(\"LR Done\")\n",
    "\n",
    "    #SVM\n",
    "    SVM_hyper_params = [ {'estimator__gamma': [0.01,0.001,0.0001],\n",
    "                          'estimator__C': [1, 10, 100],\n",
    "                           'estimator__kernel': ['rbf','linear']}]\n",
    "    \n",
    "    SVM = SVM_model(X_train,y_train,SVM_hyper_params)\n",
    "    print(\"SVM Done\")\n",
    "    \n",
    "    #GradientBoost\n",
    "    BG_hyper_params = {\n",
    "    'estimator__max_depth': [3,5,7],\n",
    "    'estimator__max_features': np.arange(0.1,1,0.1)}\n",
    "\n",
    "    GB = Grad_model(X_train,y_train,BG_hyper_params)\n",
    "    print(\"GB Done\")\n",
    "   \n",
    "    #MLP\n",
    "    MLP_hyper_params = {\n",
    "         'estimator__hidden_layer_sizes': [(300,100,100),(200,100)],\n",
    "         'estimator__activation': ['tanh', 'relu','logistic'],\n",
    "         'estimator__solver': ['sgd', 'adam'],\n",
    "         'estimator__alpha': [0.1, 0.05]\n",
    "         } \n",
    "    MLP = MLP_model(X_train,y_train,MLP_hyper_params)\n",
    "    print(\"MLP Done\")\n",
    "    \n",
    "    #RF - did not use\n",
    "#     RF_hyper_params = {\n",
    "#     'estimator__max_depth': [3,5,7],\n",
    "#     'estimator__max_features': np.arange(0.1,1,0.1)}\n",
    "\n",
    "#     RF = RF_model(X_train,y_train,RF_hyper_params)\n",
    "#     print(\"RF Done\")\n",
    "    \n",
    "\n",
    "    empty = \"NA\"\n",
    "    LRfeat = np.argsort(np.std([LR.estimators_[0].coef_,LR.estimators_[1].coef_,LR.estimators_[2].coef_],axis = 0))[::-1]\n",
    "    LRalpha = 0\n",
    "    LR_metrics = standard_metrics(LR, X_test, y_test,X_train,y_train,LRfeat,LRalpha,label=f'LR-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,LR_metrics])\n",
    "    \n",
    "    L1feat = np.argsort(np.std([L1.estimators_[0].coef_,L1.estimators_[1].coef_,L1.estimators_[2].coef_],axis = 0))[::-1]\n",
    "    L1alpha = [L1.estimators_[0].alpha_,L1.estimators_[1].alpha_,L1.estimators_[2].alpha_]\n",
    "    L1_metrics = standard_metrics(L1, X_test, y_test, X_train,y_train,L1feat,L1alpha,label=f'L1-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,L1_metrics])\n",
    "    \n",
    "    L2feat = np.argsort(np.std([L2.estimators_[0].coef_,L2.estimators_[1].coef_,L2.estimators_[2].coef_],axis = 0))[::-1]\n",
    "    L2alpha = [L2.estimators_[0].alpha_,L2.estimators_[1].alpha_,L2.estimators_[2].alpha_]\n",
    "    L2_metrics = standard_metrics(L2, X_test, y_test,X_train,y_train,L2feat,L2alpha, label=f'L2-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,L2_metrics])\n",
    "    \n",
    "    GLfeat = np.argsort(np.std([Group.estimators_[0].coef_,Group.estimators_[1].coef_,Group.estimators_[2].coef_],axis = 0))[::-1]\n",
    "    GLalpha = [Group.estimators_[0].alpha_,Group.estimators_[1].alpha_,Group.estimators_[2].alpha_]\n",
    "    GL_metrics = standard_metrics(Group, X_test, y_test, X_train,y_train,GLfeat,GLalpha,label=f'Group Lasso-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,GL_metrics])\n",
    "    \n",
    "\n",
    "    svm_param = SVM.best_params_\n",
    "    SVM_metrics = standard_metrics(SVM, X_test, y_test, X_train,y_train,empty,svm_param,label=f'SVM-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,SVM_metrics])\n",
    "    \n",
    "    GB_params = GB.best_params_\n",
    "    GBfeat = np.argsort(np.std([GB.best_estimator_.estimators_[0].feature_importances_,GB.best_estimator_.estimators_[1].feature_importances_,GB.best_estimator_.estimators_[2].feature_importances_],axis = 0))[::-1]\n",
    "    GB_metrics = standard_metrics(GB, X_test, y_test,X_train,y_train,GBfeat,GB_params,label=f'GB-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,GB_metrics])\n",
    "    \n",
    "#     rf_param =RF.best_params_\n",
    "#     RFfeat = np.argsort(np.std([RF.best_estimator_.estimators_[0].feature_importances_,RF.best_estimator_.estimators_[1].feature_importances_,RF.best_estimator_.estimators_[2].feature_importances_],axis = 0))[::-1]\n",
    "#     RF_metrics = standard_metrics(RF, X_test, y_test,X_train,y_train,RFfeat,rf_param,label=f'RF-Stage {stage}')\n",
    "#     metrics = pd.concat([metrics,RF_metrics])\n",
    "    \n",
    "    mlp_param = MLP.best_params_\n",
    "    MLP_metrics = standard_metrics(MLP, X_test, y_test, X_train,y_train,empty,mlp_param,label=f'MLP-Stage {stage}')\n",
    "    st_metrics = pd.concat([st_metrics,MLP_metrics])\n",
    "    \n",
    "    print(\"Metrics Updated\")\n",
    "    \n",
    "    st_metrics.to_csv(f'finalproject_output_{stage}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_metrics"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
